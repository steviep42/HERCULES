# Data Management

**Summary**: The organization, sharing, and preservation of data are important aspects of analysis and research although the current methods for doing this are not satisfactory. Intermediate results and data transformations are not maintained alongside original data which impairs reproducibility. A framework(s) is needed to maintain experimental data, code, and notebooks.

## Data Hosting

The primary method of managing data involves the use of Emory Box (https://emory.account.box.com/login although no one interviewed believes Box to be ideal or particularly desirable outside of it being 1) free and 2) approved use for hosting health data. The data being hosted on Box is typically distributed to collaborators and analysts who in turn maintain personally transformed versions of the data and associated intermediate results. It is rare that those results and transformed datasets are re-integrated back alongside the original data in a manner that facilitates reproducibility. While some study data is being maintained in RedCap there are an increasing number of associated experimental data types that aren't appropriate for tracking in RedCap thus they need to be maintained separately on Box. However, the Box resource cannot be computed against. Some faculty do use R Notebooks for reproducibility but point to the problem of keeping track of results generated by others who might "touch"" the project and preserve their own scripts and transformed data on their laptop. Given that shuffling data around is largely a manual process, trying to “work back” from results to the original data is challenging.

## Project Contuniuty and The Role of The Data Manager

The interviews led to a greater awareness of the importance of the "Data Manager" and how that position has evolved. It is generally understood that a Data Manager should be adept at the collection, validation, cleaning, and management of data in anticipation of downstream analysis. However, there is also a reliance on that role to provide interpretation services and to become familiar with experimental technologies simply because these individuals are closer to the data, and coding necessary to analyze it, more so than the investigator. In effect the data manager becomes the De facto informatics representative and thus occupies a larger role than simple data management. This isn't necessarily a problem until the person departs which is not uncommon in grant funded positions wherein research personnel are viewed as temporary or short term resources. 

Such departures interfere with progress and reproducibility especially when project documentation is lost or has been informally maintained. Use of coding repositories such as [Github](https://github.com/) are a function of a given lab and there is wide variation in how (or if) labs maintain software notebooks. This is of concern given that data can occupy multiple locations throughout the project trajectory as can the code used to transform that data, thus some attempt at offering a unified architecture is important. Departures of key personnel lead to a significant loss of productivity and overall project knowledge, at least from a technical point of view, that has to be rebuilt when new personnel arrives. While this is a common problem in computationally-based research perhaps the Center can mitigate these departures by offering some educational assistance for using standard tools (e.g. Git, Wikis, issue tracking). 

## The Role of the Data Lake

The cumulative storage needs of the known HERCULES pilot projects are manageable at the terabyte level although investigators frequently relocate data to perform computation and analysis which, depending on the location (desktop or cluster), can encounter storage limits. Investigators would like to "stretch out" without concern of using "too much" space though the greater priority is having access to a well supported computational environment in conjunction with a **home base** for data that does not require frequent duplication simply to enable collaboration. HERCULES projects, taken as a whole, involve a mixture of data types including PDFs, spreadsheets, SAS Data sets, CSV, accelermoter, pollution count, sequencing, and experimental data types. While this seems challenging to accommodate as a unified collection it is better to view this as a "Data Lake"" which supports the co-existence of heterogeneous data types in a way that enables "query on demand" for purposes of exploration. 

With Data Lakes, no initial effort is made to impose an enforced, relational structure onto the larger body of data in anticipation of traditional transaction analytics activity. First, those efforts assume a well-ordered, related set of data though, taken as a whole, much of the HERCULES data is not of this type. Second, large scale interrogations might not happen at a level that justifies the initial effort required to create the data warehouse. Some of the more structured data could be hosted in Redcap but most of the data currently under consideration is unstructured or semi-structured which might be better addressed using noSQL "databases" and "query on demand" tools that exist specifically to ease exploratory analysis. For example the sleep study data which is part of Dayna Johnson's project involves air quality data alongside acceleromter data which will require co-analysis that would otherwise be difficult if the data is located in multiple places. For exploratory purposes it is relatively straight forward to create a temporary "projection" onto the larger data that yields preliminary results rather than first creating a relational schema to house the data. Obviously, investigators will use known methods and packages to conduct analysis though services such as Amazon Athena (which uses open source approaches underneath the hood) are excellent tools for accomplishing "data spelunking" at preliminary levels. 

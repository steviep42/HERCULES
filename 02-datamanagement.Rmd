# Data Management

**Summary**: The organization, sharing, and preservation of data are important aspects of analysis and research although the current methods for doing this are not satisfactory. Intermediate results and data transformations are not maintained alongside original data which impairs reproducibility. A framework(s) is needed to maintain experimental data, code, and notebooks.

## Data Hosting

The primary method of managing data involves the use of Emory Box although no one interviewed believes it to be ideal outside of it 1) being free and 2) approved for the use of hosting health data. The data being maintained on Box is typically distributed to collaborators and analysts who in turn maintain personally transformed versions of the data and associated analysis code. It is rare that these results are re-integrated back alongside the original data. While some study data is being maintained in RedCap there are an increasing number of experimental data types that aren't appropriate for RedCap and therefore need storage  elsewhere. Also, the Emory Box resource cannot be computed against (although the Box company does offer an [API access module](https://box-content.readme.io/reference) which Emory does not license or offer). Some faculty do use R and Jupyter notebooks for reproducibility but point to the problem of keeping track of notebooks and results generated by others who might "touch"" the project. Given that shuffling data around is largely a manual process, trying to “work back” from results to the original data is painful.

## Project Continuity and The Data Manager

The interviews led to a greater awareness of the importance of the "Data Manager" role and how it has evolved. It is generally understood that a Data Manager should be adept at the collection, validation, cleaning, and management of data in anticipation of downstream analysis. However, there is also a reliance on that position to provide interpretation services and to demonstrate a familiarity with experimental technologies simply because these individuals are closer to the data, and the coding necessary to analyze it, more so than the investigator. In effect the data manager becomes the *De facto* informatics representative and thus occupies a much larger role than simple data management. This isn't necessarily a problem until the person departs which is not uncommon in grant funded positions.

Such departures interfere with progress and reproducibility especially when project documentation is lost or has been informally maintained. Use of coding repositories such as [Github](https://github.com/) are a function of a given lab's interest and there is wide variation in how (or if) labs maintain software notebooks. This is of concern given that data can occupy multiple locations throughout the project trajectory as can the code used to transform that data, thus some attempt at offering a unified architecture is important. Departures of key personnel lead to a significant loss of productivity and overall project knowledge, at least from a technical point of view, that has to be rebuilt when new personnel arrives. While this is a common problem in computationally-based research perhaps the Center can mitigate these departures by offering some educational assistance for using standard tools (e.g. Git, Wikis, issue tracking). 

## The Role of the Data Lake

The cumulative storage needs of the known HERCULES pilot projects are manageable at the terabyte level although investigators frequently relocate data to perform computation and analysis which, depending on the location (desktop or cluster), can encounter storage limits. Investigators would like to "stretch out" without concern of using "too much" space though the greater priority is having access to a well supported computational environment in conjunction with a **home base** for data that does not require frequent relocation and duplication simply to enable collaboration. HERCULES projects, taken as a whole, involve a mixture of data types including PDFs, spreadsheets, SAS Data sets, CSV, accelerometer, pollution count, sequencing, and experimental data types. While this seems challenging to accommodate as a unified collection it is better to view this as a "Data Lake" which supports the co-existence of heterogeneous data types in a way that enables "query on demand" for purposes of exploration. 

With Data Lakes, no initial effort is made to impose an enforced, relational structure onto the larger body of data in anticipation of traditional transaction analytics activity. First, those efforts assume a well-ordered, related set of data though, taken as a whole, much of the HERCULES data is not of this type. Second, large scale interrogations might not happen at a level that justifies the initial effort required to create the data warehouse. Some of the more structured data could be hosted in Redcap but most of the data currently under consideration is unstructured or semi-structured which might be better addressed using noSQL "databases" and "query on demand" tools that exist specifically to ease exploratory analysis. For example the sleep study data which is part of Dayna Johnson's project involves air quality data alongside accelerometer data which will require co-analysis that would otherwise be difficult if the data is located in multiple places. For exploratory purposes it is relatively straight forward to create a temporary "projection" onto the larger data that yields preliminary results rather than first creating a relational schema to house the data. Obviously, investigators will use known methods and packages to conduct analysis though services such as Amazon Athena (which uses open source approaches underneath the hood) are excellent tools for accomplishing "data spelunking" at preliminary levels. 

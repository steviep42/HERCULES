[
["index.html", "Hercules Exposome Informatics Report Chapter 1 Preface 1.1 Background 1.2 The Interview Process 1.3 Executive Summary and Initial Recommendations: 1.4 Related Topic Areas", " Hercules Exposome Informatics Report Steve Pittard 2019-02-27 Chapter 1 Preface 1.1 Background The original scope of this report was focused on first understanding the research being proposed and conducted by HERCULES affiliated investigators. In the course of the interview process it became apparent that there was significant concern about how the underlying institutional and school infrastructure has been architected (or not) to facilitate data acquisition, management, analysis, and the reproducibility of research results. Associated issues relate to interactions with Core facilities, software literacy and generally how investigators should prepare themselves and their teams to better handle data as well as the results emanating from collaborators and Core facilities, which increasingly requires greater facility with the opensource tools, the “command line”, and common software pipeline architectures. As it reltes to computation, while the promise of an institutional cloud computing environment is very appealing, most investigators have only a superficial idea about how one would fully exploit such services and how to effectively write these resources into grant proposals. From an administrative point of view the Center would like to see Pilot projects evolve into successful grant applications which would ensure ongoing support from HERCULES stakeholders. 1.2 The Interview Process Conducting these interviews was a smooth process as those contacted were very forthcoming with helpful responses that were consistent across the topic areas described in this report. Everyone was collegial and supplied viewpoints that will generally benefit the HERCULES Center as a whole. Everyone seems to be aware that Pilot grants should translate into successful grant applications both for the benefit of the investigator’s career and in support of Center aims. The interview list included Carmen Marsit, Amber Burt, Stefanie Sarnat, Howard Chang, Dayna Johnson, Michelle Kegler, Melanie Pearson, Eberhard Voit, Eddie Morgan, Donghai Liang, and Yang Liu. 1.3 Executive Summary and Initial Recommendations: Note that these suggestions represent starting points which could selectively be implemented in an a la carte fashion although an integrated approach with other HERCULES initiatives and/or those at the institutional level should be considered - particularly in conjunction with the School of Nursing. Most of these recommendations are motivated by direct suggestions from faculty who have attempted to address these problems in various ways including leveraging personal, professional, and external relationships. Further discussion is warranted particularly on items relating to the formation of a Data Science support resource and the possible licensing of a data management and computation framework such as DNANexus. Sections 2-5 provide amplifications of the recommendations. 1.3.1 Data Hosting and Management The Center should consider using Amazon S3 as a default storage solution as well as an optional layered framework for data management and computation to better organize Center information. Ideally, the institution would provide subsidized S3 access commensurate to that which is currently provided to Emory Box. Data hosting and accessibility are very important aspects of research although the current practices for supporting these activities are less than ideal. There are now three Emory storage solutions (Isilon, Box, and Amazon) and the use cases for each are not always clear to researchers. The reference data copies for many projects reside in Emory Box from which multiple copies are “fanned out” based on the number of project participants. Because of this, intermediate / final results and associated code are usually not reintegrated alongside the original data which impairs reproducibility. Unfortunately, Emory Box cannot be computed against though it is free which is why it has become the default repository for many faculty. To reduce confusion and to provide computational “elbow room”, the Center should consider using Amazon S3 as a default solution as well as an optional layered framework for data management and computation to better organize Center information. This could take several forms with the most basic being adoption of Amazon S3 storage as a default hosting solution for Center sponsored data. Ideally, the institution would provide subsidized S3 access commensurate to that which is currently provided to Emory Box. This would 1) encourage adoption of cloud computing which is a stated institutional aim and 2) increase the likelihood of adoption of cloud computing by investigators since their datq is in close proximity to compute resources. Using Amazon S3 (or Google storage) is useful because it is a highly available and reliable object store that allows for intuitive partitioning according to business unit, data type, and application. Accessing this storage from other Amazon (or Google) resources is easy and well integrated into the entire suite of services on offer. 1.3.2 Using Data Lakes The Center should consider a Data Lake strategy (described in greater detail in section 2.3) in conjunction with Amazon S3. This would allow for “query-on-demand”&quot; exploratory activity without the need to first create a highly structured database that would be expensive to create. Given that Center projects will support a large number of data types, there is little to be gained by architecting and imposing structure on information that comes from a variety of sources, in different formats, and at different “velocities”. However, simply stockpiling data in one spot, while a step in the right direction, would not immediately result in scientific insight thus some awareness of how it is organized is required to animate it. HERCULES projects involve a mixture of data types including PDFs, spreadsheets, SAS Data sets, CSV, accelermoter, pollution count, sequencing, and experimental data types. While this seems challenging to accommodate as a unified collection it is better to view this as a “Data Lake”&quot; which supports the co-existence of heterogeneous data types in a way that enables “query on demand” for purposes of exploration. There is nothing inherently wrong with a traditional data warehouse such as the one Emory uses to host patient and health data. However, in cases where the data types are high throughput in nature and are perhaps evolving over time or accumulating at rapid speeds then the Data Lake provides the best fit. The differences between a traditional Data Warehouse and a Data Lake are summarized in the following table. Characteristics Data Warehouse Data Lake Data Relational, structured data Non-relational and relational from IoT devices, mobile apps, social media, and instruments Schema Designed prior to the Data Warehouse implementation (schema-on-write) Written at the time of analysis (schema-on-read) Performance / Price Fastest query results using higher cost storage Fast Query results using low-cost storage Data Quality Highly curated data that serves as the central version of the truth Any data that may or may not be curated (ie. raw data) Users Business analysts, Physicians, Researchers Researchers, Data scientists, Data developers Analytics atch reporting, Business Intelligence and visualizations Machine Learning, Predictive analytics, data discovery and profiling 1.3.3 Managed Data and Computation The Center should consider use of a tool such as DNANexus which would provide comprehensive support for managing sequencing-based projects, analytics, as well as the convenient addition of self-developed pipelines. It is layered upon Amazon compute and storage services. Another approach, currently under consideration by both Winship Cancer and The School of Nursing, involves the use of an integrated data management and computation tool such as DNANexus which leverages Amazon computer and storage while providing open support for aggressive biomedical analytic pipelines. The product also provides extensive support for individual organizations, either separately or hierarchically, to make billing and data management transparent. This would address a number of concerns of having “everything in one place” (to the extent that it is possible) along with computational results in a format that would enable reproducibility. Additionally, a solution such as DNANexus would facilitate the integration of genomic data with clinical and other phenotypic data in a secure and compliant environment. While RedCap is useful for maintaining study information, being able to link in sequencing and sample information can be challenging. 1.3.4 HERCULES Data Science Resource The formation of a HERCULES Data Scidence support group is recommended. Such a group would improve the working dynamic since HERCULES investigators would have advocates at the initial stage of a Pilot project and as it progresses. Note that this would not replace or diminish the relationships with existing University Cores but rather supplement them. Availability of professionals who understand the analytics and informatics of HERCULES sponsored projects is important though such human resources require funding. To this end, the formation of a HERCULES Data Science group is recommended. It would provide up front consultation services for developing analytics, computing, and data management strategies for Pilot projects and help facilite their success. Another role of the group would be to organize training and provide orientation for the framework mentioned in the previous section. Initial consultations would center around design and general impressions about how to proceed though once the work starts, questions would naturally emerge that might require input from a number of “experts”. The group would not attempt to mask problems that might exist at the Core level or relieve investigators of the responsibility to cultivate informatics skills within their own lab. In fact, to be successful, this group would require the participation of a motivated representative (e.g. postdocs, data managers) from the respective labs. This group would dove tail with the Environmental Health Data Sciences Core by participation in Modeling seminars that present examples, using Center Data, of well specified research paths likely to be of interest to Center investigators. In conversation with Eberhard Voit, a co-Director of the Environmental Health Data Sciences Core, he urges additional early stage consultations to define a holistic trajectory for the Pilot projects that would incorporate his Systems Biology expertise and better anticipate the downstream, associated modeling approaches likley to support an eventual successful grant application. A HERCULES Data Science core could then work to implement schema and templates, where applicable, of common workflows to guide faculty and their representatives throughout the project life cycle. This would be particularly helpful when documenting required personnel, appropriate percent effort, and anticipated service core involvement when applying for Pilot grants. Whether this support group is an “official university core” or a group local to the Center is up for debate although the interviews reveal a gap between what labs can do for themselves related to informatics and comprehension of newer experimental data types. Equally as important is the idea that such a resource could be referenced in grant applications and serve as a evidence of a formal institutional commitment to exposome-based work. 1.3.5 Software Literacy Improving general software literacy is essential as is being able to import/transform data, accomplish analytic tasks, create plots, query databases, and create digital assets. There is currently a gap in knowledge that should be addressed via a combination of formal courses and shorter, more focused types of education. The paper “Data integration in the era of omics: current and future challenges” discusses the idea that (bio)informaticians are drawn from two distinct domains: 1) those who emerging from a computer science or mathematics background who have learned enough about biology to be helpful or 2) trained biologists who, of necessity, have acquired a knowledge of programming to approach their data. While this is an organic pattern that will likely continue it is generally agreed that inter-disciplinary involvement is essential that will involve public health investigators and statisticians. At this point, both students and faculty would benefit from courses and workshops that involve the integration of computation, data, alongside with prototypical research challenges common to omics-based investigations. The nature of these courses could be divided into two related areas: 1) Introductory material relating to the mechanics of UNIX command line, basic programming, data management, and cloud access 2) Applied courses that assume some level of software literacy. For purposes of comparison, within the School of Nursing some of the faculty have taken formal courses such as the NURS 741 Intro to Data Science Course which is a semester long class designed to offer a strong foundation for Nusing-based analytics education. However, not all faculty are interested in this long of a commitment due to existing workload and have pursued self-education via Coursera and Edx. It remains uncertatin if faculty would be interested in an actual “full-on” class unless it were shorter and less involved than a for-credit course though this is where a local Data Science support group could help with mini-courses. In terms of short-term, high impact education, one excellent (and cheap) resource is to arrange for an onsite Software Carpentry session which is a one to three day workshop devoted to teaching basic lab skills for research computing. These sessions are professionally taught and include hands on labs to learn the UNIX command line, R and Python Programming, Git, SQL and Databases. These are targeted to the novice but would also serve to reinforce skills for those with prior experience. The material is open source and maintained on GitHub so we could possibly offer the course material with local teaching resources. The following graph illustrates the motivation levels for various software and command line topics after taking a Core level Software Carpentry class. The Graduate Data Science group here at Emory has already offered one of these sessions locally and is willing to participate in more. 1.3.6 Student Recruitment Deliberate efforts should be made to recruit graduate students with ability and/or interest in data manipulation and analysis since the nature of research within the school will require such a background. While no one believes that using only student “labor” to accomplish projects is reasonable, there is a goal of attracting software literate students who can “dive in” to projects at short notice. Thus, it is difficult to imagine a scenario wherein a postdoc or graduate student could be successful without having (or acquiring) some fluency with informatics and open source tools. Such skill can be developed over time though biomedical research assumes that students can clean, reshape, and transform data both prior to analysis and afterwards. However, they will still require guidance in the selection and execution of their work which in turn assumes the existence of a knowledgeable supportive community. For better or worse, the domain of biomedical and bioinformatics research requires a base level of quantitative skill and facility with software packages such as R, Python, and various open source tools. This is where an association with the HERCULES Data Science support group would be beneficial. 1.4 Related Topic Areas The following subject areas require consideration since they transcend (but still include ) HERCULES interests. Due to their general relevance to researchers within the School of Public Health it is useful to mention these concerns as part of this report. 1.4.1 Research Desktop Support HERCULES resesrchers want a higher degree of “research aware” desktop support services from Rollins IT. While learning more about Amazon is important, all of the faculty interviewed would like greater flexibility at the School desktop level with the ability to more easily install, update, and execute open source packages such as R, Python, Anaconda and associated opensource tools. While the Center faculty do not generally expect Rollins IT personnel to conduct research or to understand the intricacies of software workflows, there remains the feeling that IT could provide a higher base level of research exploration at the desk top level. The fact that the IT group is currently not oriented towards such activities should be not an ongoing rationale to keep the status quo. It is agreed that the institution should apply rigorous security data policies but not in a way that impairs reasonable access to data and computation.** Invoking the general concept of security as a means to avoid providing assistance is a concern and investigators would like to see a more nuanced approach to the application of security that recognizes the growing variety of open source tools**. Obviously, securing health related data is a priority though locking down desktops at the current level is impacting personal productivity. 1.4.2 The Promise of Cloud Computing The School should identify paths to cloud and command-line literacy via courses and training developed internally and in conjunction with Amazon Web Services. Cloud computing offers unprecedented access to scaleable, on-demand computation and storage resources in a manner that allows Emory researchers to be competitive with institutions that possess more extensive on-premise computational infrastructure. In effect, services such as Amazon and Google democratize computing by enabling access to anyone with a credit card and a willingness to learn how to leverage the environment. However, the path to productivity is not always clear and there is confusion on how to approach Amazon services even before considering how to execute at-scale bioinformatics jobs and pipelines. The language of Amazon is one of enterprise services and architecture as opposed to research computing so educational material, even at the institutional level, generally describes services conceptually when what is needed are practical workshops for teaching researchers how to leverage the strengths of Amazon in a hands on way. The Emory LITS organization is in the process of rolling out a solution (ETA 2019) to help researchers but the larger question relates to what extent the cloud team will provide “in the trenches”, hands on training and help with selecting and managing various instance types, storage resources, and databases above and beyond the architectural level. It would be useful for the LITS cloud team, Core facilities, and/or Rollins IT to offer ongoing orientation and support for adopting cloud resources. If the institution is fully committed to the cloud and would provide a base level support for orienting users, this would then simplify the layering of more formal research oriented analytics services (e.g. HERCULES Data Science group). "],
["data-management.html", "Chapter 2 Data Management 2.1 Data Hosting 2.2 Project Contuniuty and The Role of The Data Manager 2.3 The Role of the Data Lake", " Chapter 2 Data Management Summary: The organization, sharing, and preservation of data are important aspects of analysis and research although the current methods for doing this are not satisfactory. Intermediate results and data transformations are not maintained alongside original data which impairs reproducibility. A framework(s) is needed to maintain experimental data, code, and notebooks. 2.1 Data Hosting The primary method of managing data involves the use of Emory Box (https://emory.account.box.com/login although no one interviewed believes Box to be ideal or particularly desirable outside of it being 1) free and 2) approved use for hosting health data. The data being hosted on Box is typically distributed to collaborators and analysts who in turn maintain personally transformed versions of the data and associated intermediate results. It is rare that those results and transformed datasets are re-integrated back alongside the original data in a manner that facilitates reproducibility. While some study data is being maintained in RedCap there are an increasing number of associated experimental data types that aren’t appropriate for tracking in RedCap thus they need to be maintained separately on Box. However, the Box resource cannot be computed against. Some faculty do use R Notebooks for reproducibility but point to the problem of keeping track of results generated by others who might “touch”&quot; the project and preserve their own scripts and transformed data on their laptop. Given that shuffling data around is largely a manual process, trying to “work back” from results to the original data is challenging. 2.2 Project Contuniuty and The Role of The Data Manager The interviews led to a greater awareness of the importance of the “Data Manager” and how that position has evolved. It is generally understood that a Data Manager should be adept at the collection, validation, cleaning, and management of data in anticipation of downstream analysis. However, there is also a reliance on that role to provide interpretation services and to become familiar with experimental technologies simply because these individuals are closer to the data, and coding necessary to analyze it, more so than the investigator. In effect the data manager becomes the De facto informatics representative and thus occupies a larger role than simple data management. This isn’t necessarily a problem until the person departs which is not uncommon in grant funded positions wherein research personnel are viewed as temporary or short term resources. Such departures interfere with progress and reproducibility especially when project documentation is lost or has been informally maintained. Use of coding repositories such as Github are a function of a given lab and there is wide variation in how (or if) labs maintain software notebooks. This is of concern given that data can occupy multiple locations throughout the project trajectory as can the code used to transform that data, thus some attempt at offering a unified architecture is important. Departures of key personnel lead to a significant loss of productivity and overall project knowledge, at least from a technical point of view, that has to be rebuilt when new personnel arrives. While this is a common problem in computationally-based research perhaps the Center can mitigate these departures by offering some educational assistance for using standard tools (e.g. Git, Wikis, issue tracking). 2.3 The Role of the Data Lake The cumulative storage needs of the known HERCULES pilot projects are manageable at the terabyte level although investigators frequently relocate data to perform computation and analysis which, depending on the location (desktop or cluster), can encounter storage limits. Investigators would like to “stretch out” without concern of using “too much” space though the greater priority is having access to a well supported computational environment in conjunction with a home base for data that does not require frequent duplication simply to enable collaboration. HERCULES projects, taken as a whole, involve a mixture of data types including PDFs, spreadsheets, SAS Data sets, CSV, accelermoter, pollution count, sequencing, and experimental data types. While this seems challenging to accommodate as a unified collection it is better to view this as a “Data Lake”&quot; which supports the co-existence of heterogeneous data types in a way that enables “query on demand” for purposes of exploration. With Data Lakes, no initial effort is made to impose an enforced, relational structure onto the larger body of data in anticipation of traditional transaction analytics activity. First, those efforts assume a well-ordered, related set of data though, taken as a whole, much of the HERCULES data is not of this type. Second, large scale interrogations might not happen at a level that justifies the initial effort required to create the data warehouse. Some of the more structured data could be hosted in Redcap but most of the data currently under consideration is unstructured or semi-structured which might be better addressed using noSQL “databases” and “query on demand” tools that exist specifically to ease exploratory analysis. For example the sleep study data which is part of Dayna Johnson’s project involves air quality data alongside acceleromter data which will require co-analysis that would otherwise be difficult if the data is located in multiple places. For exploratory purposes it is relatively straight forward to create a temporary “projection” onto the larger data that yields preliminary results rather than first creating a relational schema to house the data. Obviously, investigators will use known methods and packages to conduct analysis though services such as Amazon Athena (which uses open source approaches underneath the hood) are excellent tools for accomplishing “data spelunking” at preliminary levels. "],
["software-literacy-1.html", "Chapter 3 Software Literacy 3.1 Concerns About Falling Behind 3.2 Open Source Tools 3.3 Attracting Software Literate Students 3.4 Source Code Maintenance 3.5 Notebooks and Reproducible Research", " Chapter 3 Software Literacy Summary: Having software literacy is essential to research success. Being able to import/transform data, accomplish analytic tasks, create plots, query databases, and create digital assets will facilitate publication and grant submissions. Many faculty would like to become more self-sufficient. 3.1 Concerns About Falling Behind Faculty have improved their knowledge via self-education though most want more experience and are frustrated with not being able to fully comprehend the code in published pipelines and/or how one might modify or extend the code to pursue ad-hoc analysis paths. Consequently, they feel “locked in” and limited by their relative lack of the UNIX command line and general programming knowledge and might not then feel confident enough to modify code for fear of “breaking” something. 3.2 Open Source Tools Some faculty have graduate training and exposure to SAS and remain attracted to the integrated nature of those tools but realize that R and Python are increasingly the more relevant languages. To this end, Coursera and Edx courses have been useful to pick up targeted knowledge though it is generally agreed that project driven coding is the best for learning and reinforcing skills. Leveraging Software Carpentry https://software-carpentry.org/lessons style courses is also a reasonable possibility since they offer 1 to 3 days workshops desgined to impart basic computation skills for researchers. 3.3 Attracting Software Literate Students There is an associated interest in attracting students who possess programming skills or are at the very least enthusiastic about a learning path that involves programming. This is both to facilitate research and to make the student experience more productive since analysis of experimental data types is likely to be a major theme in most thesis projects. It is important to note that such students are not being seen as a substitute for more formal types of support though as part of their educational experience they could work in an internal service center or core to interact with experienced professionals. Again, this is not intended to be a roundabout method of getting research projects accomplished but an attempt at accelerating student knowledge of “real world” analysis applications. 3.4 Source Code Maintenance Many of the reference software pipelines for analysis are obtained from literature references, training, and/or vignettes that might be supplied with a given software package. It is expected that changes will be made to scripts simply to accommodate local data sources which might very well lead to questions and interactions with package authors, statisticians, and more generally anyone who might be able to help the research bypass roadblocks. Thus, the state of a script or code can vary greatly at any time depending on where in the process things are. These scripts are almost always maintained on personal laptops or desktop computers although at least one person uses Git and GitHub to archive scripts. Even so, the scripts are typically highly customized for a specific project. Also, GitHub is for maintaining code changes and not data thus it is inappropriate to host data and results (intermediate or final) along with the code. It’s also true that GitHub exists to encourage collaboration and co-development though not everyone who would use Git or GitHub feels that the code is in a “good enough” state to push to a repository for general use and inspection. 3.5 Notebooks and Reproducible Research Some faculty are using R Notebooks to capture a narrative alongside the code used to generate results. These notebooks are prime candidates for registration on GitHub as it allows others to benefit from the work and permits easy retrieval of previous versions. Of course, some do not wish to share their code for various reasons such as that is in development. Another reason is that the end result is simply a minor variation of a published pipeline in response to specific sequencing problems or situations that might not relate to another PIs project. Some labs have in fact maintained scripts in GitHub although the practice is not prevalent. Reproducing research is major concern that has been placed aside in favor of simply being able to get analysis accomplished (which itself has been a challenge). In short “the data can’t grow roots anywhere” so being able to integrate all intermediate results, let alone track the provenance and chain of custody thereof, has been very challenging outside of the simplest of projects. A larger issue exists in knowing the “best practices” associated with creating reproducible resources. Some faculty do know about “notebooks” in R and Python and see the value in using them although integrating a personal notebook with that of an analyst or collaborator remains a challenge. Keeping copies of “cleaning” scripts in addition to analysis pipelines is a priority though concern remains at being able to reproduce results if and when key personnel leave the department. "],
["interaction-with-cores.html", "Chapter 4 Interaction with Cores 4.1 Expectations of Cores 4.2 Understanding Results 4.3 Need for More Consultation 4.4 Possible Solutions", " Chapter 4 Interaction with Cores Summary: Some researchers would like more up front support from Cores in addition to back end consultation to better understand results. Faculty also understand that they have a responsibility to acquire the requisite knowledge to speak intelligently about newer data types and experiments but will need help to get up to speed. 4.1 Expectations of Cores 4.1.0.1 The Knowledge Gap - Communication With Cores Core facilities, both local and external, are challenged to produce actionable results at a reasonable cost though clients of these facilities (Microbiome and Metabolomics in particular) typically want more in terms of explanation and follow-up support as they progress towards publishable conclusions. While most of those interviewed felt comfortable with Core results being returned from Cores, some had yet to fully engage these services so could not comment on the experience. One investigator, Donghai Liang, reports satisfaction with the Metabolomics core though points out that he has worked extensively with this type of data during his Post doc experience so he has perspective that others do not. Non coincidentally, he is asked to assist with questions about metabolomics data and while he is willing to help he is increasingly engaged with his new duties. The Cores generally do not have the personnel or time to offer ongoing support which has produced something of a gap that is usually addressed by drafting in resources such as post docs, Georgia Tech students, collaborators, and peers at other institutions. Addressing this gap involves a combination of activities involving the development of domain expertise by investigators (or their proxy) along with greater willingness by the Cores to treat engagements holistically as opposed to “one-and-done” transactions. This is why HERCULES should consider the formation of a data science support group to perform the necessary triage and hand offs between the group. Note that this support group isn’t intended to be a “drop off” service that insulates investigators from the realities of data analysis and Core products but rather a junction point where concerns can be captured and addressed according to Center priorities and resources. 4.2 Understanding Results The following figure captures the essence of the challenges when interacting with Core facilities. The best situation occurs when there is a strong intersection of knowledge between the facility and the investigator to the point wherein questions and concerns are easily addressed. Both sides have a sufficient background to ask pointed questions about software pipelines used in the process as well as some knowledge of the domain from which the data was generated (e.g. Microbiome, Metabolomics, Lipidomics). The second case represents the case wherein the investigator has some knowledge of at least one of the areas but needs significantly more help in other areas. This situation is more common than the first. There is light intersection of shared knowledge which subsequently provokes more questions (sometimes more advanced, sometimes basic and remedial). 4.3 Need for More Consultation Additional consultation from the Cores is required given that researchers might not be well trained in a specific technique or field. This is true for both study and experimental design as well as result interpretation. Additionally, being able to point to vital working relationships to local Cores is very helpful when applying for grants. In absence of that type of relationship then external providers must be considered. One problem with core facilities is the recruitment and retention of the necessary domain expertise to complete a number of projects on time. Typically, core facilities are staffed with junior level faculty who are on their way to other career opportunities or staff who are working (many times temporarily) for less than market salaries perhaps because a partner or spouse is employed elsewhere within the institution. Lack of advancement and funding for professional development can dissuade otherwise qualified personnel from pursuing such positions. Universities such as Vanderbilt have begun to establish job descriptions and promotion and tenure policies relevant to the issue of career tracks for core directors and personnel. 4.4 Possible Solutions In response to faculty input the following figure represents a type of interaction with internal cores that can help. It assumes that supplemental expertise can be assigned or mobilized in parallel (to the extent that can currently be achieved) to fully leverage results coming back from a Core. Some investigators might already have this relationship in some form (e.g. postdocs, junior faculty working on the project) but it’s not a common scenario nor is it one that is realistic for junior faculty. The following figure represents a distillation of input from faculty wherein there is ready access to a group of integrated professionals who can manage interactions (where requested) and fulfill some of the needs that is typically pushed back onto the investigator. In this scenario investigators still retain the ability to go directly to a Core or leverage the strengths of the internal Core who can manage relationships, organize training, and intelligently allocate personnel for various tasks. Obviously, this will involve recruitment and possible reorganization of existing resources (those who aren’t already fully funded). "],
["computation.html", "Chapter 5 Computation 5.1 Data Management and Computation 5.2 Cloud vs Local Resources 5.3 Desktop Support for Open Source Analysis Tools", " Chapter 5 Computation Summary: Computation underlies research analytics and faculty use whatever resources they can find. Lots of code and data winds up on the laptops of collaborators and students which works against reproducibility, security, and project continuity. 5.1 Data Management and Computation Smaller projects can in fact be completed using laptops and desktops though the trend has rapidly moved towards more samples per project thus being able to easily and conveniently compute against multiple data sources is very important. This is a limitation of Emory Box in that the data cannot be directly accessed via APIs (Application Programming Interfaces). It might be more convenient for it to be in Amazon S3 bucket or a folder attached to a compute node, or in a system such as DNANexus or Seven Bridges both of which are enterprise frameworks for data management, analysis, and computation. These are currently being considered by WCI in support of their ORIEN http://oriencancer.org/#about project though it is quite possible that HERCULES could benefit from using the license also since it would provide a standard tool for management, analysis, and compute. However, embracing these tools would still involve formal training and orientation to ensure productivity. 5.2 Cloud vs Local Resources All of those interviewed are aware of resources such as Amazon or Google but lack the training to reasonably approach these resources directly unless “chaperoned” by someone more knowledgeable or by involving a funded collaborator. A minority of those interviewed express an interest in knowing how to configure cloud resources at a “nuts and bolts” level though most do not see immediate value in that. Although they do understand that being more facile at the UNIX command line and with R and Python can only help them. In the end, faculty are not fixated on a specific architecture or method of computation as long as it is readily available and accessible using standard, documented methods. Many faculty are getting their computational work done “by hook or by crook” so they are open to a standard, well supported setup that doesn’t require them to “jump around” to different resources throughout the project life cycle. 5.2.1 Onboarding Cloud Support On boarding to the Cloud is a problem. The cloud’s ability to match the scope of most any research project is indeed a true convenience which, however, requires significant knowledge to fully exploit. Knowing how to responsibly provision storage, “spin up” instances, create databases, and effectively close out projects are important skills. It has been difficult getting a clear message from the institution as to when a service offering would be official and what precisely the support mechanism would provide particularly as it relates to hands on support and assistance with managing cloud-based instances. A soft opening was announced for Fall of 2018 though the service arrival date is now “early 2019”. Investigators will need assistance and/or proxies to handle the foundational work which would typically involve students although a more official, permanent, and reliable form of support is essential since students and post docs eventually move on. In absence of enthusiastic end-user support, Amazon or Google will likely not be heavily used except by those already knowledgeable and/or funded specifically to the do such work. At this point, investigators have the impression that they are expected to “self-educate” though they would like a base level of support from LITS or Rollins IT upon which more specific forms of research support could be built. 5.2.2 Post Project Data Archival Research evolves over time and involves input from collaborators (statisticians, analysts, bioinformatics etc.) which means that the data must remain resident on the cloud as the project evolves. This ongoing occupancy involves costs and while someone could move the data off to cheaper storage and then move it back to EBS, this is inconvenient and also interrupts continuity. Being able to maintain data on the Amazon cloud is an interest especially with the attractive life cycle management tools that can push data out to cheaper levels of storage and, in terms of archival data, Amazon Glacier. Additionally, Amazon provides versioning, flexible permission schemes, and tiers for infrequently accessed data though intelligent use of these features will require support from the institution. It is not apparent that LITS is supporting this direction given that the preferred storage platforms are Emory Box and the Isilon storage systems. Putting data in Box is “okay” but 1) it cannot be computed against and 2) if the institution is truly moving towards a cloud-first solution then investigators should be allocated Amazon S3 storage equivalent in size to the free Emory Box offering. 5.3 Desktop Support for Open Source Analysis Tools Faculty have expressed concern at the level of IT desktop support they receive which is limited when considering that they need to conduct analyses on their desktops using open source tools. Being able to install and update opensource tools (e.g. R, Python, add-on packages) and receive troubleshooting assistance is a common need although it has been challenging to obtain such help. While faculty (or their proxy) use the Rollins cluster there will always be a need for desktop computation simply to try things out and/or do trial runs of pipelines found in literature. To this end having more liberal policies for installing tools on the desk top is desired. Note that faculty are NOT expecting the IT group to provide research advice or debug programming issues. However, they should be able to troubleshoot basic installation problems and be able to assist in the updating of packages and generally be aware of desktop issues as it relates to research. "]
]

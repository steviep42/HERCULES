[
["index.html", "Hercules Exposome Informatics Report Chapter 1 Preface 1.1 Background 1.2 The Interview Process 1.3 Summary and Recommendations: 1.4 Related Topics", " Hercules Exposome Informatics Report Steve Pittard 2019-02-28 Chapter 1 Preface 1.1 Background The original scope of this report was focused on understanding the research being proposed and conducted by HERCULES affiliated investigators. In the course of the interview process it became apparent that there was concern about how the underlying institutional and school infrastructure has been architected (or not) to facilitate data acquisition, management, analysis, and the reproducibility of research results. Associated issues relate to interactions with Core facilities, software literacy and generally how investigators should prepare themselves and their teams to handle data emanating from collaborators and Core facilities, which increasingly requires facility with the opensource tools and computational architectures. While the promise of an institutional cloud computing environment is very appealing, most investigators have only a superficial idea about how one would fully exploit such services and how to effectively write these resources into grant proposals. From an administrative point of view the Center would like to see Pilot projects evolve into successful grant applications which would ensure ongoing support from HERCULES stakeholders. 1.2 The Interview Process Conducting these interviews was a smooth process as those contacted were very forthcoming with helpful responses that were consistent across the topic areas described in this report. Everyone was collegial and supplied viewpoints that will generally benefit the HERCULES Center as a whole. Everyone seems to be aware that Pilot grants should translate into successful grant applications both for the benefit of the investigator’s career and in support of Center aims. The interview list included Carmen Marsit, Amber Burt, Stefanie Sarnat, Howard Chang, Dayna Johnson, Michelle Kegler, Melanie Pearson, Eberhard Voit, Eddie Morgan, Donghai Liang, and Yang Liu. 1.3 Summary and Recommendations: Note that these suggestions represent starting points which could be implemented in an a la carte fashion although an integrated approach with other HERCULES initiatives and/or those at the institutional level should be considered - particularly in conjunction with the School of Nursing. Most of these recommendations are motivated by direct suggestions from faculty who have attempted to address problems in various ways including leveraging personal, professional, and external relationships. Note that these recommendations are not presented in a specific order or priority. Sections 2-5 provide additional perspective on the recommendations. 1.3.1 Data Hosting and Management The Center should consider using Amazon S3 as a default storage solution as well as an optional layered framework (See 1.3.3) on top of that for data and computational management. It would be ideal for the institution to provide subsidized S3 access commensurate to that which is currently provided to the Emory Box service. Data hosting and accessibility are very important aspects of research although the current practices for supporting these activities are less than ideal. There are now three Emory storage solutions (Isilon, Box, and Amazon) and the use cases for each are not always clear to researchers. The reference data sets for many projects reside in Emory Box from which multiple copies are “fanned out” based on the number of project participants. Because of this, i results and associated code are usually not reintegrated alongside the original data which impairs reproducibility. Unfortunately, Emory Box cannot be computed against though it is free which is why it has become the default repository for many faculty. To reduce confusion and to provide computational “elbow room”, the Center should consider using Amazon S3 as a default solution as well as an optional layered framework for data management and computation to better organize Center information. This could take several forms with the most basic being adoption of Amazon S3 storage as a default hosting solution for Center sponsored data. Ideally, the institution would provide subsidized S3 access commensurate to that which is currently provided to Emory Box. This would 1) encourage adoption of cloud computing which is a stated institutional aim and 2) increase the likelihood of cloud computing adoption since project data would be in close proximity to compute resources. Using Amazon S3 (or Google storage) is useful because it is a highly available and reliable object store that allows for intuitive partitioning according to business unit, data type, and application. Accessing this storage from other Amazon (or Google) resources is easy and well integrated with the entire suite of services on offer. 1.3.2 Using “Data Lakes” The Center should consider a Data Lake strategy (described in greater detail in section 2.3) in conjunction with Amazon S3. This would allow for “query-on-demand”&quot; exploratory activity without the need to first create a highly structured data warehouse. HERCULES projects involve a mixture of data types including PDFs, spreadsheets, SAS Data sets, CSV, accelerometer, pollution count, sequencing, and experimental data types. While this seems challenging to accommodate as a unified collection, it is better to view the catalogue as a “Data Lake” which supports the co-existence of heterogeneous data types in a way that simplifies exploration. It is useful to think of a Data Lake in terms of a “landing spot” for disparate sources of data that might be in some way related or, eventually related, depending on downstream transformations. This approach eliminates the need to impose an up front, rigid structure on the data catalogue which over time might experience the addition of new data types possibly in real time. The Data Lake convention enables a “query-on-demand” capability for large bodies of data - both static and dynamic. Technologies such as Apache Spark can then be employed to provided familiar structures to users, such as data frames, in a way that insulates them from the complexities of the underlying infrastructure. While there is nothing inherently “wrong” with a traditional data warehouse such as the one(s) Emory uses to host patient and health data, the “Data Lake” paradigm is the most appropriate approach for HERCULES since the data types are many and varied. Some of the projects currently being pursued do not involve particularly large data sets and can be managed using desktops or laptops in which case there is no need to move beyond that level. However, when considering the larger aims of HERCULES, the Data Lake structure has a role. The differences between a traditional Data Warehouse and a Data Lake are summarized in the following table. Characteristics Data Warehouse Data Lake Data Highly relational, structured and transformed Non relational and relational from IoT devices, mobile devices, instruments, Schema Designed/created prior to the Data Warehouse implementation (schema-on-write) Created at the time of analysis (schema-on-read) Performance / Price Fast query results using higher cost storage Fast query results using low-cost storage Data Quality Highly cleaned and curated Any data (ie. raw data), untransformed End Users Business analysts, Physicians, Researchers Researchers, Data scientists, Data developers, Anyone Use Cases Batch reporting, Business Intelligence and visualizations Machine Learning, Predictive Analytics, Data Discovery Security Restricted, Strong Authentication (health data) Open, Democratic (usually) The McKinsey organization presents a graphic which documents how spontaneous exploration and investigation can occur without a lot of “heavy lifting” by the scientist tasked with extracting meaning from an increasing data catalogue. It is helpful to think of Data Lakes as being a continuum of activities wherein one can engage the data using very basic approaches (download and analyze) or sophisticated approaches such as automated analysis processes that get triggered into action when a certain data type arrives from a real time data collection point. The benefit of the Data Lake is that explorations can be initiated by the investigator without larger IT involvement (unless desired) and human resources can be drafted in over time to develop specific applications on an as-needed basis. 1.3.3 Managed Data and Computation The Center should consider use of a tool such as DNANexus which would provide comprehensive support for managing sequencing-based projects, analytics, as well as the convenient addition of self-developed pipelines. It is layered upon Amazon compute and storage services. Another approach, currently under consideration by both Winship Cancer and The School of Nursing, involves the use of an integrated data management and computation tool such as DNANexus which leverages Amazon computer and storage while providing an intuitive interface along with support for aggressive biomedical analytic pipelines. The product also provides extensive support for individual organizations, either separately or hierarchically, to make billing and data management transparent. This would address a number of concerns of having “everything in one place” (to the extent that it is possible) along with computational results in a format that would enable reproducibility. Additionally, a solution such as DNANexus would facilitate the integration of genomic data with clinical and other phenotypic data in a secure and compliant environment. While RedCap is useful for maintaining study information, being able to link in sequencing and sample information can be challenging. 1.3.4 HERCULES Data Science Resource The formation of a HERCULES Data Science support group is recommended to faciliate project completion. At a minimum, the Center should offer an enhanced form of project management to 1) inform the Pilot Project Director of barriers to progress and to 2) ensure completion of Pilot Projects in compliance with Center aims. Availability of professionals who understand the analytics and informatics components of HERCULES projects is important though these resources require funding. To this end, the formation of a HERCULES Data Science group should be considered. (Note that “Data Science” is a general term employed here primarily to distinguish the intent of the group). It would provide up front consultation services for developing analytics, computing, and data management strategies for Pilot projects and help facilitate their success. Another role of the group would be to organize training and provide orientation for the framework mentioned in the previous section (1.3.3) and generally help with software literacy education (1.3.5). Initial consultations would center around design and impressions about how to proceed though once the work starts, questions would naturally emerge that might require input from a number of “experts”. The group would not attempt to mask problems that might currently exist at the Core level or relieve investigators of the responsibility to cultivate informatics skills within their own lab. In fact, to be successful, this group would require the participation of a motivated representative (e.g. postdocs, data managers) from the respective labs. This group would dove tail with the Environmental Health Data Sciences Core by participation in Modeling seminars that present examples of well specified research paths likely to be of interest to Center investigators. In conversation with Eberhard Voit, a co-Director of the Environmental Health Data Sciences Core, he urges early stage consultations to define a holistic trajectory for the Pilot projects that would incorporate Systems Biology expertise to anticipate downstream modeling approaches likely to support a successful grant application. A HERCULES Data Science core could then work to implement templates, where applicable, of common workflows to guide faculty and their representatives throughout the project life cycle. This would be particularly helpful when documenting required personnel, appropriate percent effort, and anticipated service core involvement when applying for Pilot grants. Whether this support group is an “official university core” or a group local to the Center is up for debate although the interviews reveal a gap between what labs can do for themselves related to informatics and comprehension of newer experimental data types. However, not everyone feels that a group is necessary in that knowing how to work with data and write code is the responsibility of the investigator. Having facility with informatics might be how an investigator would distinguish themselves and would make their funding proposals more attractive. On the other hand, it is generally agreed that need for informatics support is prevalent just that some investigators feel that cultivating informatics skills within one’s lab will provide independence and insulation from whatever support policies the institution at large might (or not) implement. 1.3.5 Software Literacy Improving general software literacy is essential as is being able to import/transform data, accomplish analytic tasks, create plots, query databases, and create digital assets. There is currently a gap in knowledge that should be addressed via a combination of formal courses and shorter, more focused types of education. The paper “Data integration in the era of omics: current and future challenges” discusses the idea that (bio)informaticians are drawn from two distinct domains: 1) those who emerging from a computer science or mathematics background who have learned enough about biology to be helpful or 2) trained biologists who, of necessity, have acquired a knowledge of programming to approach their data. While this has been the tradition, clearly other disciplines have entered the scene to the extent that there is a ubiquitous, generic need for software and coding literacy. Both students and faculty would benefit from courses and workshops that involve prototypical informatics challenges common to omics-based research. The nature of these courses could be divided into two related areas: 1) Introductory material relating to the mechanics of UNIX command line, basic programming, data management, and cloud access 2) Applied courses that assume some level of software literacy. For purposes of comparison, within the School of Nursing some of the faculty have taken formal courses such as the NURS 741 “Intro to Data Science Course” which is a semester long class designed to offer a strong foundation for analytics of biomedical data. However, not all faculty are interested in this long of a commitment due to existing workload and have pursued self-education via Coursera and Edx. It remains uncertain if faculty would be interested in an actual “full-on” class unless it were shorter and less involved though this is where a local Data Science support group could help with mini-courses. In terms of short-term, high impact education, one excellent (and cheap) resource is to arrange for an onsite Software Carpentry session which is a one to three day workshop devoted to teaching basic skills for research computing. These sessions are professionally taught and include hands on labs to learn the UNIX command line, R and Python Programming, Git, SQL and Databases. These are targeted to the novice but would also serve to reinforce skills for those with prior experience. The material is open source and maintained on GitHub so we could possibly offer the course material with local teaching resources. The following graph illustrates the motivation levels for various software and command line topics after taking a Core level Software Carpentry class. The Graduate Data Science group here at Emory has already offered one of these sessions locally and is willing to participate in more. 1.3.6 Student Recruitment Deliberate efforts should be made to recruit graduate students with ability and/or interest in data manipulation and analysis since the nature of research within the Center will require such a background. There is a goal of attracting software literate students who can “dive in” to projects at short notice. It is difficult to imagine a scenario wherein a postdoc or graduate student could be successful without having (or acquiring) some fluency with informatics and open source tools. Such skill can be developed over time though biomedical research assumes that students can clean, reshape, and transform data both prior to analysis and afterwards. However, they will still require guidance in the selection and execution of their work which in turn assumes the existence of a knowledgeable supportive community. This is where an association with the HERCULES Data Science support group would be beneficial. 1.4 Related Topics The following subject areas require consideration although they are not uniquely related to HERCULES interests. However, because of their relevance to HERCULES researchers within the School of Public Health it is useful to consider them but not at the expense of Center aims. 1.4.1 Research Desktop Support HERCULES resesrchers want a higher degree of “research aware” desktop support services from Rollins IT. While learning more about Amazon is important, all of the faculty interviewed would like greater flexibility at the School desktop level with the ability to more easily install, update, and execute open source packages such as R, Python, Anaconda and associated opensource tools. While the Center faculty do not generally expect Rollins IT personnel to conduct research or to understand the intricacies of software workflows, there remains the feeling that IT could provide a higher base level of research exploration at the desk top level. The fact that the IT group is currently not oriented towards such activities should be not be an ongoing rationale to keep the status quo. It is agreed that the institution should apply rigorous security data policies but not in a way that impairs reasonable access to data and computation.However, invoking the general concept of security as a means to avoid providing assistance is a concern and investigators would like to see a more nuanced approach to the application of security that recognizes their computational needs. Obviously, securing health related data is a priority though locking down desktops at the current level is impacting personal productivity. 1.4.2 The Promise of Cloud Computing The School should identify paths to cloud and command-line literacy via courses and training developed internally and in conjunction with Amazon Web Services. It is important to know to what extent LITS and Rollins IT will directly be supporting AWS at a practical level. Specifically, will they be perfoming on-boarding or do they expect users to self-educate ? Cloud computing offers unprecedented access to scale-able, on-demand computation and storage resources in a manner that allows Emory researchers to be competitive with institutions that possess more extensive on-premise computational infrastructure. In effect, services such as Amazon and Google democratize computing by enabling access to anyone with a credit card and a willingness to learn how to leverage the environment. However, the path to productivity is not always clear and there is confusion on how to approach Amazon services even before considering how to execute at-scale bioinformatics jobs and pipelines. The language of Amazon is one of enterprise services and architecture as opposed to research computing so educational material, even at the institutional level, generally describes services conceptually when what is needed are practical workshops for teaching researchers how to apply the strengths of Amazon in a hands on way. The Emory LITS organization is in the process of rolling out a solution (ETA 2019) to help researchers but the larger question relates to what extent the cloud team will provide “in the trenches”, hands on training and help with selecting and managing various instance types, storage resources, and databases above and beyond the architectural level. It would be useful for the LITS cloud team, Core facilities, and/or Rollins IT to offer ongoing orientation and support for adopting cloud resources. This would then simplify the layering of a more formal research oriented analytics services (e.g. HERCULES Data Science group). While we shouldn’t expect IT to engage in research it is reasonable to expect them to educate users at a level that would enable them to be productive with Amazon which is a stated direction for the institution. The HERCULES Center could adopt a policy of “ground up” training that would take users from “zero to hero” relative to cloud use though that would involve more of a commitment (both human and financial) since it would be in addition to research bioinformatics interests. Being able to make assumptions about the level of commitment offered by LITS and Rollins IT would be helpful to plan more effectively though the recommendations described in this documented could initiated independently and adjusted over time. "],
["data-management.html", "Chapter 2 Data Management 2.1 Data Hosting 2.2 Project Continuity and The Data Manager 2.3 The Role of the Data Lake", " Chapter 2 Data Management Summary: The organization, sharing, and preservation of data are important aspects of analysis and research although the current methods for doing this are not satisfactory. Intermediate results and data transformations are not maintained alongside original data which impairs reproducibility. A framework(s) is needed to maintain experimental data, code, and notebooks. 2.1 Data Hosting The primary method of managing data involves the use of Emory Box although no one interviewed believes it to be ideal outside of it 1) being free and 2) approved for the use of hosting health data. The data being maintained on Box is typically distributed to collaborators and analysts who in turn maintain personally transformed versions of the data and associated analysis code. It is rare that these results are re-integrated back alongside the original data. While some study data is being maintained in RedCap there are an increasing number of experimental data types that aren’t appropriate for RedCap and therefore need storage elsewhere. Also, the Emory Box resource cannot be computed against (although the Box company does offer an API access module which Emory does not license or offer). Some faculty do use R and Jupyter notebooks for reproducibility but point to the problem of keeping track of notebooks and results generated by others who might “touch”&quot; the project. Given that shuffling data around is largely a manual process, trying to “work back” from results to the original data is painful. 2.2 Project Continuity and The Data Manager The interviews led to a greater awareness of the importance of the “Data Manager” role and how it has evolved. It is generally understood that a Data Manager should be adept at the collection, validation, cleaning, and management of data in anticipation of downstream analysis. However, there is also a reliance on that position to provide interpretation services and to demonstrate a familiarity with experimental technologies simply because these individuals are closer to the data, and the coding necessary to analyze it, more so than the investigator. In effect the data manager becomes the De facto informatics representative and thus occupies a much larger role than simple data management. This isn’t necessarily a problem until the person departs which is not uncommon in grant funded positions. Such departures interfere with progress and reproducibility especially when project documentation is lost or has been informally maintained. Use of coding repositories such as Github are a function of a given lab’s interest and there is wide variation in how (or if) labs maintain software notebooks. This is of concern given that data can occupy multiple locations throughout the project trajectory as can the code used to transform that data, thus some attempt at offering a unified architecture is important. Departures of key personnel lead to a significant loss of productivity and overall project knowledge, at least from a technical point of view, that has to be rebuilt when new personnel arrives. While this is a common problem in computationally-based research perhaps the Center can mitigate these departures by offering some educational assistance for using standard tools (e.g. Git, Wikis, issue tracking). 2.3 The Role of the Data Lake The cumulative storage needs of the known HERCULES pilot projects are manageable at the terabyte level although investigators frequently relocate data to perform computation and analysis which, depending on the location (desktop or cluster), can encounter storage limits. Investigators would like to “stretch out” without concern of using “too much” space though the greater priority is having access to a well supported computational environment in conjunction with a home base for data that does not require frequent relocation and duplication simply to enable collaboration. HERCULES projects, taken as a whole, involve a mixture of data types including PDFs, spreadsheets, SAS Data sets, CSV, accelerometer, pollution count, sequencing, and experimental data types. While this seems challenging to accommodate as a unified collection it is better to view this as a “Data Lake” which supports the co-existence of heterogeneous data types in a way that enables “query on demand” for purposes of exploration. With Data Lakes, no initial effort is made to impose an enforced, relational structure onto the larger body of data in anticipation of traditional transaction analytics activity. First, those efforts assume a well-ordered, related set of data though, taken as a whole, much of the HERCULES data is not of this type. Second, large scale interrogations might not happen at a level that justifies the initial effort required to create the data warehouse. Some of the more structured data could be hosted in Redcap but most of the data currently under consideration is unstructured or semi-structured which might be better addressed using noSQL “databases” and “query on demand” tools that exist specifically to ease exploratory analysis. For example the sleep study data which is part of Dayna Johnson’s project involves air quality data alongside accelerometer data which will require co-analysis that would otherwise be difficult if the data is located in multiple places. For exploratory purposes it is relatively straight forward to create a temporary “projection” onto the larger data that yields preliminary results rather than first creating a relational schema to house the data. Obviously, investigators will use known methods and packages to conduct analysis though services such as Amazon Athena (which uses open source approaches underneath the hood) are excellent tools for accomplishing “data spelunking” at preliminary levels. "],
["software-literacy-1.html", "Chapter 3 Software Literacy 3.1 Concerns About Falling Behind 3.2 Open Source Tools 3.3 Attracting Software Literate Students 3.4 Source Code Maintenance 3.5 Notebooks and Reproducible Research", " Chapter 3 Software Literacy Summary: Having software literacy is essential to research success. Being able to import/transform data, accomplish analytic tasks, create plots, query databases, and create digital assets will facilitate publication and grant submissions. Many faculty would like to become more self-sufficient. 3.1 Concerns About Falling Behind Faculty have improved their knowledge via self-education though most want more experience and are frustrated with not being able to fully comprehend the code in published pipelines and/or how one might modify or extend the code to pursue ad-hoc analysis paths. Consequently, they feel “locked in” and limited by their relative lack of the UNIX command line and general programming knowledge and might not then feel confident enough to modify code. 3.2 Open Source Tools Many faculty have graduate training and exposure to SAS and remain attracted to the integrated nature of those tools but realize that R and Python are increasingly the more relevant languages. To this end, Coursera and Edx courses have been useful to pick up targeted knowledge though it is generally agreed that project driven coding is the best for learning and reinforcing skills. Leveraging Software Carpentry https://software-carpentry.org/lessons style courses is also a reasonable possibility since they offer 1 to 3 days workshops designed to impart basic computation skills for researchers. 3.3 Attracting Software Literate Students There is an associated interest in attracting students who possess programming skills or are enthusiastic about a learning path that involves programming. This is both to facilitate research and to make the student experience more productive since analysis of experimental data is likely to be a major theme in most thesis projects. It is important to note that such students are not being seen as a substitute for more formal types of informatics support though as part of their educational experience they could work in an internal service center or core to interact with experienced professionals. Again, this is not intended to be a roundabout method of getting research projects accomplished but an attempt at accelerating student knowledge of “real world” analysis applications in a way that benefits the lab. 3.4 Source Code Maintenance Many of the popular analysis pipelines are obtained from literature references, training, and/or vignettes that might be supplied with a given software package. It is expected that changes will be made to scripts simply to accommodate local data sources which might very well lead to questions and interactions with package authors, statisticians, and more generally anyone who might be able to help the research bypass roadblocks. Thus, the state of a script or code can vary greatly at any time depending on where in the process things are. These scripts are almost always maintained on personal laptops or desktop computers although at least one person uses Git and GitHub to archive scripts. Even so, the scripts are typically highly customized for a specific project. Also, GitHub is for maintaining code changes and not data thus it is inappropriate to host data and results (intermediate or final) along with the code. It’s also true that GitHub exists to encourage collaboration and co-development though not everyone who would use Git or GitHub feels that the code is in a “good enough” state to push to a repository for general use and inspection. However, using Git is essential in this day and age for anyone who is working on a digitial asset involving code since it is the “gold standard” for software development, hosting, and distribution. 3.5 Notebooks and Reproducible Research Some faculty are using R Notebooks to capture a narrative alongside the code used to generate results. These notebooks are prime candidates for registration on GitHub as it allows others to benefit from the work and permits easy retrieval of previous versions. Of course, some do not wish to share their code for various reasons such as that is in development. Another reason is that the end result is simply a minor variation of a published pipeline in response to specific sequencing problems or situations that might not relate to another PI’s project. Some labs have in fact maintained scripts in GitHub although the practice is not prevalent. Reproducing research is major concern that has been placed aside in favor of simply being able to get analysis accomplished (which itself has been a challenge). In short “the data can’t grow roots anywhere” so being able to integrate all intermediate results, let alone track the provenance and chain of custody thereof, has been very challenging outside of the simplest of projects. A larger issue exists in knowing the “best practices” associated with creating reproducible resources. Some faculty do know about “notebooks” in R and Python and see the value in using them although integrating a personal notebook with that of an analyst or collaborator remains a challenge. Keeping copies of “cleaning” scripts in addition to analysis pipelines is a priority though concern remains at being able to reproduce results if and when key personnel leave the department. "],
["interaction-with-cores.html", "Chapter 4 Interaction with Cores 4.1 Communication With Cores 4.2 Domain Expertise vs Remedial Knowledge", " Chapter 4 Interaction with Cores 4.1 Communication With Cores Researchers would benefit from an open-ended support option from Core facilities. Core facilities are challenged to produce actionable results at a reasonable cost though clients of these facilities (Microbiome and Metabolomics in particular) may want follow-up support as they progress towards publishable conclusions. Faculty understand that they have a responsibility to acquire the requisite knowledge to speak intelligently about newer data types but will sometimes need help as they get up to speed. While most of those interviewed felt comfortable with Core results, some had yet to fully engage these services so could not comment on the experience whereas others wanted the option of more followup. One investigator, Donghai Liang, reports satisfaction with the Metabolomics core though points to his prior experience with this type of data during his PhD work. Non coincidentally, he is sometimes asked to assist with interpretation issues and, while he is willing to help, he is increasingly engaged with his new duties as a Research Professor. 4.2 Domain Expertise vs Remedial Knowledge Cores generally do not have the personnel to provide ongoing support outside of the initial engagement. Core rates are generally transaction-based when what investigators sometimes desire is a consultation service that, depending on the effort involved, would involve a per-hour, “attorney” style engagement which might add considerably to the cost of Core use. When faced with this scenario investigators might seek to use their network to complete the work or hire their own resources. Obviously, the best situation occurs when there is a strong intersection of knowledge between the Core facility and the investigator. Both sides have the requisite background to ask pointed questions about software pipelines and the experimental techniques used in the process as well as some knowledge of the domain from which the data was generated. The second case is where the investigator has some common knowledge with the Core yet still needs help working with the results or wishes to explore alternative scenarios that would then require facility with the pipelines used to generate the initial results. The Core might assume that the investigator already has such knowledge and might then view such requests as remedial especially if their point of intersection is a proxy for the investigator such as a student. So, satisfaction with the Core might then become a function of how willing it is to provide this level of support. One possible solution would be some reliance on the Data Science group mentioned in 1.3.4 though it depends on the nature of the question. If it relates to facility with languages and data reshaping or transformation then this might be useful though if it relates to expertise with the experimental data type then the Core remains the best source of information. In either case, the investigator must offer a motivated representative to work through these issues as neither group can take on these issues in whole unless of course they are being compensated specifically for that effort. "],
["computation.html", "Chapter 5 Computation 5.1 Data Management and Computation 5.2 Cloud vs Local Resources 5.3 Desktop Support for Open Source Analysis Tools", " Chapter 5 Computation Summary: Computation underlies research analytics and faculty use whatever resources they can find. Lots of code and data winds up on the laptops of collaborators and students which works against reproducibility, security, and continuity. 5.1 Data Management and Computation Smaller projects can in fact be completed using laptops and desktops though the trend has rapidly moved towards more samples per project thus being able to conveniently compute against multiple data sources is very important. This is a limitation of Emory Box in that the data cannot be directly accessed via APIs (Application Programming Interfaces). It might be more convenient for it to be in an Amazon S3 bucket or a folder attached to a compute node, or in a system such as DNANexus which is an enterprise framework for data management, analysis, and computation. This is currently being considered by WCI in support of their ORIEN http://oriencancer.org/#about project though it is possible that HERCULES could share the license though it would still involve formal training and orientation to ensure productivity. 5.2 Cloud vs Local Resources All of those interviewed are aware of resources such as Amazon or Google but lack the training to reasonably approach these resources directly unless “chaperoned” by someone more knowledgeable or by involving a funded collaborator. A minority of those interviewed express an interest in knowing how to configure cloud resources at a “nuts and bolts” level though most do not see immediate value in that. Although they do understand that being more facile at the UNIX command line and with R and Python can only help them. In the end, faculty are not fixated on a specific architecture or method of computation as long as it is readily available and accessible using standard, documented methods. Many faculty are getting their computational work done “by hook or by crook” so they are open to a standard, well supported setup that doesn’t require them to “jump around” to different resources throughout the project life cycle. 5.2.1 Onboarding Cloud Support On boarding to the Cloud is a problem. The cloud’s ability to match the scope of most any research project is indeed a true convenience which, however, requires significant knowledge to fully exploit. Knowing how to responsibly provision storage, “spin up” instances, create databases, and effectively close out projects are important skills. It has been difficult getting a clear message from the institution as to when a service offering would be official and what precisely the support mechanism would provide particularly as it relates to hands on support and assistance with managing cloud-based instances. A soft opening was announced for Fall of 2018 though the service arrival date is now “early 2019”. Investigators will need assistance and/or proxies to handle the foundational work which would typically involve students although a more official, permanent, and reliable form of support is essential since students and post docs eventually move on. In absence of enthusiastic end-user support, Amazon or Google will likely not be heavily used except by those already knowledgeable and/or funded specifically to the do such work. At this point, investigators have the impression that they are expected to “self-educate” though they would like a base level of support from LITS or Rollins IT upon which more specific forms of research support could be built. 5.2.2 Post Project Data Archival Research evolves over time and involves input from collaborators (statisticians, analysts, bioinformatics etc.) which means that the data must remain resident on the cloud as the project evolves. This ongoing occupancy involves costs and while someone could move the data off to cheaper storage and then move it back to EBS, this is inconvenient and also interrupts continuity. Being able to maintain data on the Amazon cloud is an interest especially with the attractive life cycle management tools that can push data out to cheaper levels of storage and, in terms of archival data, Amazon Glacier. Additionally, Amazon provides versioning, flexible permission schemes, and tiers for infrequently accessed data though intelligent use of these features will require support from the institution. It is not apparent that LITS is supporting this direction given that the preferred storage platforms are Emory Box and the Isilon storage systems. Putting data in Box is “okay” but 1) it cannot be computed against and 2) if the institution is truly moving towards a cloud-first solution then investigators should be allocated Amazon S3 storage equivalent in size to the free Emory Box offering. 5.3 Desktop Support for Open Source Analysis Tools Faculty have expressed concern at the level of IT desktop support they receive which is limited when considering that they need to conduct analyses on their desktops using open source tools. Being able to install and update opensource tools (e.g. R, Python, add-on packages) and receive troubleshooting assistance is a common need although it has been challenging to obtain such help. While faculty (or their proxy) use the Rollins cluster there will always be a need for desktop computation simply to try things out and/or do trial runs of pipelines found in literature. To this end having more liberal policies for installing tools on the desk top is desired. Note that faculty are NOT expecting the IT group to provide research advice or debug programming issues. However, they should be able to troubleshoot basic installation problems and be able to assist in the updating of packages and generally be aware of desktop issues as it relates to research. "]
]
